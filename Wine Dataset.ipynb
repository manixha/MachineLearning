{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOM9MUMZaq2GaWB8n7+jFsJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manixha/MachineLearning/blob/main/Wine%20Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Boosting Techniques"
      ],
      "metadata": {
        "id": "ghC6niNGJsZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AdaBoost Technique\n",
        "\n",
        "Adaptive boosting or AdaBoost is one of the simplest boosting algorithms. Usually, decision trees are used for modelling. Multiple sequential models are created, each correcting the errors from the last model. AdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly.\n",
        "\n",
        "Below are the steps for performing the AdaBoost algorithm:\n",
        "\n",
        "Initially, all observations in the dataset are given equal weights.\n",
        "A model is built on a subset of data.\n",
        "Using this model, predictions are made on the whole dataset.\n",
        "Errors are calculated by comparing the predictions and actual values.\n",
        "While creating the next model, higher weights are given to the data points which were predicted incorrectly.\n",
        "Weights can be determined using the error value. For instance, higher the error more is the weight assigned to the observation.\n",
        "This process is repeated until the error function does not change, or the maximum limit of the number of estimators is reached."
      ],
      "metadata": {
        "id": "IIoE-abBI-XB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine"
      ],
      "metadata": {
        "id": "rp2yi9IdI8sT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = load_wine().data\n",
        "y = load_wine().target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "qwhQnFy2C_Ul"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "ada_boost = AdaBoostClassifier(random_state=1)\n",
        "ada_boost.fit(X_train, y_train)\n",
        "ada_boost.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJn88qECDUeT",
        "outputId": "92df55f8-cd3a-49c3-f1b1-7225550630ad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8333333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradient Boosting Technique\n",
        "\n",
        "Gradient Boosting or GBM\n",
        "\n",
        "It is another ensemble machine learning algorithm that works for both regression and classification problems. GBM uses the boosting technique, combining a number of weak learners to form a strong learner. Regression trees used as a base learner, each subsequent tree in series is built on the errors calculated by the previous tree."
      ],
      "metadata": {
        "id": "Qg3cu6UAG2dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "2sGg-jiGDZsd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad_boost= GradientBoostingClassifier(learning_rate=0.01,random_state=1)\n",
        "grad_boost.fit(X_train, y_train)\n",
        "grad_boost.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WrSJztAHEk5",
        "outputId": "17954431-7529-427d-c94f-642308ba9498"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9444444444444444"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##XGBoost Technique\n",
        "\n",
        "XGBoost (extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. XGBoost has proved to be a highly effective ML algorithm, extensively used in machine learning competitions and hackathons. XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known as ‘regularized boosting‘ technique.\n",
        "\n",
        "Let us see how XGBoost is comparatively better than other techniques:\n",
        "\n",
        "Regularization: Standard GBM implementation has no regularisation like XGBoost. Thus XGBoost also helps to reduce overfitting.\n",
        "\n",
        "Parallel Processing: XGBoost implements parallel processing and is faster than GBM .\n",
        "XGBoost also supports implementation on Hadoop.\n",
        "High Flexibility: XGBoost allows users to define custom optimization objectives and evaluation criteria adding a whole new dimension to the model.\n",
        "\n",
        "Handling Missing Values: XGBoost has an in-built routine to handle missing values.\n",
        "\n",
        "Tree Pruning: XGBoost makes splits up to the max_depth specified and then starts pruning the tree backwards and removes splits beyond which there is no positive gain.\n",
        "\n",
        "Built-in Cross-Validation: XGBoost allows a user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run."
      ],
      "metadata": {
        "id": "nMJBQNwYHXV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "A5KuQasHHlMU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_boost=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
        "xgb_boost.fit(X_train, y_train)\n",
        "xgb_boost.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65SX9BxFHL5N",
        "outputId": "a49af6ea-4a3e-450f-874a-8f3d9f9a1dc3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9444444444444444"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}